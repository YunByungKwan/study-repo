# Elastic Search - 6

D-Day: D +13
날짜: Nov 21, 2020
종료 여부: No
최종수정일: Nov 21, 2020 9:45 PM

## 6. 고급 검색

---

단순한 검색기능 외에도 하이라이트 기능이나 한글 형태소 분석등 제공하는 고급 검색 기능에 대해 Araboza.

`목차`

1. 한글 형태소 분석기 사용하기
2. 검색 결과 하이라이트하기
3. 스크립팅을 이용해 동적으로 필드 추가하기
4. 검색 템플릿을 이용한 동적 쿼리 제공
5. 별칭을 이용해 항상 최신 인덱스 유지하기
6. 스냅숏을 이용한 백업과 복구

## 6.1 한글 형태소 분석기 사용하기

---

한글은 조사나 어미의 접미사가 명사, 동사등과 결합하여 분석이 쉽지 않으나

오픈소스로 공개되어 많이 사용되고 있는 

- 은전한닢 형태소 분석기
- 트위터 형태소 분석기
- Nori 형태소 분석기

를 파헤쳐보자

### 6.1.1 은전한닢 형태소 분석기

---

- Mecab-ko-dic 기반의 분석기

    (mecab-ko-dic은 오픈 소스 형태소 분석 엔진인 MeCab을 사용하여, 한국어 형태소 분석을 하기 위한 프로젝트입니다)

- 엘라스틱에서 가장 보편적으로 사용하는 한글 형태소 분석기

```bash
PUT /seunjeon_default_analyzer
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1,
    "index": {
      "analysis": {
        "analyzer": {
          "korean": {
            "type": "custom",
            "tokenizer": "seunjeon_default_tokenizer"
          }
        },
        "tokenizer": {
          "seunjeon_default_tokenizer": {
            "type": "seunjeon_tokenizer",
            "index_eojeol": false,
            "user_words": [
              "낄끼+빠빠,-100", "c\\+\\+", "어그로", "버카충", "abc마트"
            ]
          }
        }
      }
    }
  }
}

# 결과
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "seunjeon_default_analyzer"
}
```

대표적인 옵션들

- user_words
    - 사용자 사전을 정의
- user_dict_path
    - 사용자 사전 파일의 경로 정의. 엘라스틱 서치의 config 폴더 밑에 생성
- decompoud
    - 복합명사 분해 여부를 정의 (default : True)
- deinflect
    - 활용어의 원형을 추출 (default : True)
- index_eojeol
    - 어절 추출 (default : True)
- index_poses
    - 추출할 품사를 정의.
- post_tagging
    - 품사 태깅 여부를 정의 (default : True)
- max_unk_length
    - Unknown 품사 태깅의 키워드로 뽑을 수 있는 최대 길이를 정의한다 (default : 8)

품사태그는 알아서 보셈.

### 사전 추가

---

한글에는 복합명사가 있어 "박준뿡뿡이" 같은 단어는 박준과 뿡뿡이라는 형태소로 분리되는 단어다.

이를 두개를 다르게 검색했을때도 검색되게 하려면 복합명사를 분리해서 역색인 해야한다.

이를 위해는 사용자가 등록하는 사전이 필요한데 이를 **사용자 사전**이라 한다.

```bash
PUT /seunjeon_with_dic_analyzer
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer": {
          "seunjeon_default_tokenizer": {
            "index_eojeol": "false",
            "pos_tagging": "false",
            "user_dict_path": "dic/user_dic.csv",
            "type": "seunjeon_tokenizer"
          }
        },
        "analyzer": {
          "korean": {
            "filter": [
              "lowercase"
            ],
            "tokenizer": "seunjeon_default_tokenizer",
            "type": "custom"
          }
        }
      }
    }
  }
}

# 결과
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "seunjeon_with_dic_analyzer"
}
```

사용자 사전시에는 다음과 같은 형식으로 등록한다.

```bash
박준뿡뿡이,-100
박준,-50
뿡뿡이,-50
```

### 6.1.2 Nori 형태소 분석기

---

하나의 토크나이저와 두개의 토큰 필터로 구성

- nori_tokenizer
- nori_part_of_speech
- nori_readingform

### 6.1.2.1 nori_tokenizer 토크나이저

토크나이저는 형태소를 토큰 형태로 분리하는데 사용. 파라미터는 2가지 지원

- decompound_mode: 복합명사를 토크나이적 ㅏ처리하는 방식
- user_dictionary : 사용자 사전 정의

**1) decompound_mode**

- none
    - 복합명사로 분리하지 않는다 ex) 월미도 영종도
- discard
    - 복합명사로 분리하고 원본 데이터는 삭제 한다. ex)잠실역 ⇒ 잠실 , 역
- mixed
    - 복합명사로 분리하고 원본 데이터는 유지한다. ex)잠실역 ⇒ 잠실,역,잠실역

**2)user_dictionary**

- mecab-ko-dic 사용

```bash
PUT nori_analyzer
{
  "settings": {
    "index": {
      "analysis": {
        "tokenizer": {
          "nori_user_dict_tokenizer": {
            "type": "nori_tokenizer",
            "decompound_mode": "mixed",
            "user_dictionary": "userdict_ko.txt"
          }
        },
        "analyzer": {
          "nori_token_analyzer": {
            "type": "custom",
            "tokenizer": "nori_user_dict_tokenizer"
          }
        }
      }
    }
  }
}

# 결과
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "nori_analyzer"
}
```

```bash
POST nori_analyzer/_analyze
{
  "analyzer": "nori_token_analyzer",
  "text": "박준뿡뿡이"
}

# 결과
{
    "tokens": [
        {
            "token": "박준뿡뿡이",
            "start_offset": 0,
            "end_offset": 3,
            "type": "word",
            "position": 0,
            "positionLength": 2
        },
        {
            "token": "박준",
            "start_offset": 0,
            "end_offset": 2,
            "type": "word",
            "position": 0
        },
        {
            "token": "뿡뿡이",
            "start_offset": 2,
            "end_offset": 3,
            "type": "word",
            "position": 1
        }
    ]
}
```

### 6.1.2.2 nori_part_of_speech 토큰 필터

- 품사 태그 세트와 일치하는 토큰을 찾아 제거하는 필터.
- 역색인 될 명사를 선택적으로 고르기 가능.
- stoptags 파라미터를 통해 제거할 형태소를 지정하는 것이 가능

```bash
PUT nori_analyzer/_settings
{
  "index": {
    "analysis": {
      "analyzer": {
        "nori_stoptags_analyzer": {
          "tokenizer": "nori_tokenizer",
          "filter": [
            "nori_posfilter"
          ]
        }
      },
      "filter": {
        "nori_posfilter": {
          "type": "nori_part_of_speech",
          "stoptags": [
            "E",
            "IC",
            "J",
            "MAG",
            "MAJ",
            "MM",
            "NA",
            "NR",
            "SC",
            "SE",
            "SF",
            "SH",
            "SL",
            "SN",
            "SP",
            "SSC",
            "SSO",
            "SY",
            "UNA",
            "UNKNOWN",
            "VA",
            "VCN",
            "VCP",
            "VSV",
            "VV",
            "VX",
            "XPN",
            "XR",
            "XSA",
            "XSN",
            "XSV"
          ]
        }
      }
    }
  }
}
```

- 명사를 제외한 모든 형태소 제거 예시

```bash
POST nori_analyzer/_analyze
{
  "analyzer": "nori_token_analyzer",
  "text": "그대 이름은 장미"
}

# 결과
{
    "tokens": [
        {
            "token": "그대",
            "start_offset": 0,
            "end_offset": 2,
            "type": "word",
            "position": 0
        },
        {
            "token": "이름",
            "start_offset": 3,
            "end_offset": 5,
            "type": "word",
            "position": 1
        },
        {
            "token": "장미",
            "start_offset": 7,
            "end_offset": 9,
            "type": "word",
            "position": 3
        }
    ]
}
```

### 6.1.2.3 nori_readingform 토큰 필터

문서에 존재하는 한자를 ㅎ나글로 변경하는 역할

```bash
PUT nori_readingform
{
    "settings": {
        "index":{
            "analysis":{
                "analyzer" : {
                    "nori_readingform_analyzer" : {
                        "tokenizer" : "nori_tokenizer",
                        "filter" : ["nori_readingform"]
                    }
                }
            }
        }
    }
}

# 결과
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "nori_readingform"
}
```

### 6.1.3 트위터 형태소 분석기

정규화, 토큰화,스테밍,어구 추출 가능

- 정규화
    - 입니닼ㅋㅋ → 입니다 ㅋㅋ
- 토큰화
    - 한국어를 처리하는 예시입니다 → 한국어,를,처리,하는,예시,입니다
- 스테밍
    - 한국어를 처리하는 예시입니다 → 한국어,를,처리,하는,예시,이다
- 어구추출
    - 한국어를 처리하는 예시입니다 → 한국어,처리,예시,처리하는 예시

**사전추가**

사전 외의 단어를 직접 추가하기도 가능. 띄어쓰기는 불가능.

**인덱스 설정**

플러그인 컴포넌트

- Character Filter
- Tokenizer
- Token Filter
- Analyzer

## 6.2 검색 결과 하이라이트하기

---

웹상에서 출력할 때 사용자가 입력한 검색어를 강조하는 기능

```bash
PUT movie_highlighting/_doc/1
{
  "title": "Harry Potter and the Deathly Hallows"
}
```

하이라이트 사용

```bash
POST movie_highlighting/_search
{
   "query": {
       "match": {
	       "title": {
	           "query": "harry"
	       }
       }
   },
   "highlight": {
       "fields" : {
          "title" : {}
       }
   }
}

# 결과
{
    "took": 102,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.2876821,
        "hits": [
            {
                "_index": "movie_highlighting",
                "_type": "_doc",
                "_id": "1",
                "_score": 0.2876821,
                "_source": {
                    "title": "Harry Potter and the Deathly Hallows"
                },
                "highlight": {
                    "title": [
                        "<em>Harry</em> Potter and the Deathly Hallows"
                    ]
                }
            }
        ]
    }
}
```

하이라이트 태그 변경하는 것도 가능

```bash
POST movie_highlighting/_search
{
  "query": {
    "match": {
      "title": {
        "query": "harry"
      }
    }
  },
  "highlight": {
    "pre_tags": [
      "<strong>"
    ],
    "post_tags": [
      "</strong>"
    ],
    "fields": {
      "title": {}
    }
  }
}

# 결과
{
    "took": 8,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.2876821,
        "hits": [
            {
                "_index": "movie_highlighting",
                "_type": "_doc",
                "_id": "1",
                "_score": 0.2876821,
                "_source": {
                    "title": "Harry Potter and the Deathly Hallows"
                },
                "highlight": {
                    "title": [
                        "<strong>Harry</strong> Potter and the Deathly Hallows"
                    ]
                }
            }
        ]
    }
}
```

### 6.3 스크립팅을 이용해 동적으로 필드 추가하기

---

스크립트를 이용해 사용자가 특정 로직을 삽입하는 것이 가능하다.

- config 폴더에 스크립팅 저장
- In-requests 방식: 동적 스크립팅 API 호출시 코드 내에 스크립트를 직접 지정
- 스크립팅 전용 언어 Painless
- _update API와 함께 사용

### 6.4 검색 템플릿을 이용한 동적 쿼리 제공

---

검색 템플릿의 필드명과 파라미터를 사용해서 쿼리를 전송하고 템플릿에 제공한 파라미터로 실제 검색이 이뤄진다.

- 복잡한 검색 로직을 템플릿으로 저장하고 활용할 수 있다.
- 검색 템플릿은 Mustache라는 템플릿 엔진을 사용해서 표현

검색 템플릿의 필드명 + 파라미터 사용

- 쿼리전송하기
    - 템플릿 제공 파라미터로 실제 검색 이뤄짐
- 검색 템플릿은 Mustache 활용
- 복잡한 검색 로직을 템플릿으로 저장하고 활용할 수 있음.

### 6.5 별칭을 이용해 항상 최신 인덱스 유지하기

---

- 인덱스 변경에 대응 가능

- 멀티테넌시 형태를 활용하여 인덱스를 관리하는 방법
- 색인을 다시 만드는 경우에 더 많이 활용

### 6.6 스냅숏을 이용한 백업과 복구

---

- 스냅숏 기능을 이용해 개별 인덱스를 백업할 수도 있고 클러스터 전체를 스냅숏으로 만드는 것도 가능
    1. 리포지토리 생성: 스냅숏들을 저장하는 논리적인 공간
    2. 스냅숏 생성: 스냅숏 대상이 되는 인덱스는 더 이상 변경이 없는 인덱스